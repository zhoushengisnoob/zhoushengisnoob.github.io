{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "import torch_scatter.scatter as scatter \n",
    "from torch_geometric.utils import degree,softmax\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from copy import deepcopy \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph数据结构（存储方式）\n",
    "对于图数据我们首先为图数据定义一种存储形式。下面是一种常见的定义方式：\n",
    "- 节点属性 x：形状为(num_nodes,feature_dim)的矩阵\n",
    "- 图结构 edge_index：以COO格式存储graph的邻接矩阵，形状为(2,num_edges)，第一行为源节点，第二行为目的节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self,feat,edge_index) -> None:\n",
    "        self.x = torch.tensor(feat)\n",
    "        self.edge_index = torch.tensor(edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN框架\n",
    "GNN模型的实现关键在于实现GNN的Message Passing，即通过对节点和边的特征进行聚合来更新节点的特征。\n",
    "Message Passing 可以分为三步：\n",
    "1. 消息构建（Message）：对于每个节点，根据邻居节点的特征计算出一条消息；\n",
    "2. 聚合（Aggregate）：将所有接收到的消息进行聚合\n",
    "3. 更新（Update）：根据聚合后的信息,和节点本身的信息更新节点的特征\n",
    "\n",
    "下面是一个简单的GNN框架，我们可基于这个框架实现GCN、GAT、GraphSage等GNN模型。\n",
    "更加全面的框架可以参考PyG和DGL，我们只需要实现框架中的message，aggregate，update三个函数就能够完成一个GNN layer，同时我们给出了基于这个框架实现的GCN，GAT和GraphSAGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagePassing(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x,edge_index):\n",
    "        out = self.propagate(x,edge_index)\n",
    "        return out\n",
    "\n",
    "    def propagate(self, x, edge_index):\n",
    "        msg = self.message(x,edge_index)\n",
    "        agg = self.aggregate(msg,edge_index)\n",
    "        out = self.update(x,agg,edge_index)\n",
    "        return out\n",
    "\n",
    "    def message(self,x,edge_index):\n",
    "        '''\n",
    "        构建需要传递的消息\n",
    "        '''\n",
    "        src,dst = edge_index\n",
    "        return x[dst]\n",
    "\n",
    "    def aggregate(self,msg,edge_index):\n",
    "        \n",
    "        '''\n",
    "        汇集来自邻居的消息\n",
    "        '''\n",
    "        src,dst = edge_index\n",
    "        out = scatter(msg, src, dim=0, reduce='sum') # 聚合邻居节点的信息 \n",
    "        return out\n",
    "    \n",
    "    def update(self,x,agg,edge_index):\n",
    "        '''\n",
    "        更新节点特征\n",
    "        '''\n",
    "        return agg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  GCN\n",
    "- GCN的消息传递机制为 $\\mathbf{x}_i^{(k)}=\\sum_{j\\in\\mathcal{N}(i)\\cup\\{i\\}}\\frac1{\\sqrt{\\deg(i)}\\cdot\\sqrt{\\deg(j)}}\\cdot\\left(\\mathbf{W}^\\top\\cdot\\mathbf{x}_j^{(k-1)}\\right)+\\mathbf{b}$\n",
    "- message：对邻居特征进行线性变换后归一化$\\frac1{\\sqrt{\\deg(i)}\\cdot\\sqrt{\\deg(j)}}\\cdot\\left(\\mathbf{W}^\\top\\cdot\\mathbf{x}_j^{(k-1)}\\right)$\n",
    "- aggregate：使用SumPooling汇聚邻居消息\n",
    "- update：加上偏置项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self,in_dim,out_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim,out_dim,bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim).float())\n",
    "\n",
    "    def message(self,x,edge_index):\n",
    "        '''\n",
    "        构建需要传递的消息\n",
    "        '''\n",
    "        x = self.linear(x) # 对邻居节点特征进行特征变换\n",
    "        src,dst = edge_index  \n",
    "        node_degree = degree(src)  \n",
    "        norm = 1/torch.sqrt(node_degree[src]*node_degree[dst])\n",
    "        norm[norm==float('inf')]=1\n",
    "        msg = x[dst] * norm.view(-1,1)\n",
    "        return msg\n",
    "\n",
    "    def update(self, x, agg, edge_index):\n",
    "        return agg + self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAT\n",
    "- GAT的消息传递机制为：$\\vec{h}_i^{\\prime}=\\sigma\\left(\\sum_{j\\in\\mathcal{N}_1}\\alpha_{ij}\\mathbf{W}\\vec{h}_j\\right)$\n",
    "- 其中$\\alpha_{i j}=\\operatorname{softmax}_{j}\\left(e_{i j}\\right)=\\frac{\\exp \\left(e_{i j}\\right)}{\\sum_{k \\in N_{i}} \\exp \\left(e_{i k}\\right)}=\\frac{\\exp \\left(\\operatorname{LeakyReLU}\\left(\\overrightarrow{\\mathbf{a}}^{T}\\left[\\mathbf{W} \\vec{h}_{i} \\| \\mathbf{W} \\vec{h}_{j}\\right]\\right)\\right)}{\\sum_{k \\in \\mathcal{N}_{i}} \\exp \\left(\\operatorname{LeakyReLU}\\left(\\overrightarrow{\\mathbf{a}}^{T}\\left[\\mathbf{W} \\vec{h}_{i} \\| \\mathbf{W} \\vec{h}_{k}\\right]\\right)\\right)}$\n",
    "- message:带attention权重的消息\n",
    "- aggregate：Sumpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATConv(MessagePassing):\n",
    "    def __init__(self,in_dim,out_dim):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.a = nn.Parameter(torch.rand(size=(2*out_dim, 1)))\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "    \n",
    "    def message(self, x, edge_index):\n",
    "        Wh = self.linear(x)\n",
    "        Wh1 = torch.matmul(Wh, self.a[:self.out_dim, :])\n",
    "        Wh2 = torch.matmul(Wh, self.a[self.out_dim:, :])\n",
    "        src,dst = edge_index\n",
    "        e = self.leakyrelu(Wh1[src] + Wh2[dst])\n",
    "        norm = softmax(e,index=src)\n",
    "        return norm.view(-1,1) * Wh[dst]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphSAGE\n",
    "\n",
    "- 消息传递机制为$h_v^{(l)} = W_l\\cdot h_v^{(l-1)} + W_r \\cdot AGG(\\{h_u^{(l-1)}, \\forall u \\in N(v) \\})$\n",
    "- message：原始的属性\n",
    "- aggregate：根据原始论文，可以为SumPooling，MeanPooing，MaxPooling等\n",
    "- update：分别处理节点自身消息和邻居后求和\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEConv(MessagePassing):\n",
    "    def __init__(self,in_dim,out_dim,pooling = 'sum'):\n",
    "        super().__init__()\n",
    "        self.linear_l = nn.Linear(in_dim,out_dim)\n",
    "        self.linear_r = nn.Linear(in_dim,out_dim) \n",
    "        self.pooling = pooling\n",
    "    def aggregate(self, msg, edge_index):\n",
    "        src,dst = edge_index\n",
    "        if self.pooling == 'sum':\n",
    "            agg = scatter(msg, src, dim=0, reduce='sum') \n",
    "        elif self.pooling == 'mean':\n",
    "            agg = scatter(msg, src, dim=0, reduce='mean') \n",
    "        elif self.pooling == 'max':\n",
    "            agg = scatter(msg, src, dim=0, reduce='max') \n",
    "        return agg \n",
    "    \n",
    "    def update(self, x, agg, edge_index):\n",
    "        out = self.linear_l(x) + self.linear_r(agg) \n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 节点分类任务\n",
    "- 节点分类任务是针对图数据的节点进行分类。通常具有transductive和inductive两种设置。其中transductive设置下，模型对所有节点均可见，而inductive设置下，模型仅对训练集上的节点可见。\n",
    "- 节点分类任务的输入为节点的特征向量和图的结构，输出为节点的类别标签。\n",
    "- 节点分类任务的评价指标有准确率、F1值等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、载入数据集以Cora数据集为例\n",
    "- 利用pyg的api，将cora数据集加载到内存中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='data', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhousheng/anaconda3/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "feat = dataset.data.x \n",
    "label = dataset.data.y\n",
    "edge_index = dataset.data.edge_index\n",
    "# 使用预定义好的数据集划分\n",
    "train_mask = dataset.data.train_mask\n",
    "val_mask = dataset.data.val_mask\n",
    "test_mask = dataset.data.test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2、构建GNN模型\n",
    "- 利用我们之前实现的MessagePassing搭建GNN模型\n",
    "- 以GCN为例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self,in_dim,hid_dim,out_dim,num_layers=2,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(GCNConv(in_dim,hid_dim))\n",
    "        for _ in range(num_layers-2):\n",
    "            self.layers.append(GCNConv(hid_dim,hid_dim))\n",
    "        self.layers.append(GCNConv(hid_dim,out_dim))\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x,edge_index):\n",
    "        h = x \n",
    "        for layer in self.layers[:-1]:\n",
    "            h = self.act(layer(h,edge_index))\n",
    "            h = self.dropout(h) \n",
    "        h = self.layers[-1](h,edge_index)         \n",
    "        return h\n",
    "\n",
    "model = GNN(feat.shape[1],64,max(label)+1,num_layers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3、训练模型\n",
    "- 节点分类常用的训练损失为CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 1.9454, Val Accuracy 0.3900\n",
      "Epoch 1, Loss 1.9081, Val Accuracy 0.4580\n",
      "Epoch 2, Loss 1.8070, Val Accuracy 0.4900\n",
      "Epoch 3, Loss 1.6526, Val Accuracy 0.4820\n",
      "Epoch 4, Loss 1.4614, Val Accuracy 0.5300\n",
      "Epoch 5, Loss 1.2138, Val Accuracy 0.5980\n",
      "Epoch 6, Loss 0.9608, Val Accuracy 0.6880\n",
      "Epoch 7, Loss 0.7208, Val Accuracy 0.7220\n",
      "Epoch 8, Loss 0.5187, Val Accuracy 0.7700\n",
      "Epoch 9, Loss 0.3539, Val Accuracy 0.7820\n",
      "Epoch 10, Loss 0.2267, Val Accuracy 0.7840\n",
      "Epoch 11, Loss 0.1444, Val Accuracy 0.7740\n",
      "Epoch 12, Loss 0.0985, Val Accuracy 0.7760\n",
      "Epoch 13, Loss 0.0757, Val Accuracy 0.7660\n",
      "Epoch 14, Loss 0.0453, Val Accuracy 0.7500\n",
      "Epoch 15, Loss 0.0336, Val Accuracy 0.7520\n",
      "Epoch 16, Loss 0.0192, Val Accuracy 0.7620\n",
      "Epoch 17, Loss 0.0179, Val Accuracy 0.7580\n",
      "Epoch 18, Loss 0.0104, Val Accuracy 0.7540\n",
      "Epoch 19, Loss 0.0062, Val Accuracy 0.7540\n",
      "Epoch 20, Loss 0.0029, Val Accuracy 0.7500\n",
      "Epoch 21, Loss 0.0023, Val Accuracy 0.7400\n",
      "Epoch 22, Loss 0.0013, Val Accuracy 0.7380\n",
      "Epoch 23, Loss 0.0011, Val Accuracy 0.7360\n",
      "Epoch 24, Loss 0.0020, Val Accuracy 0.7360\n",
      "Epoch 25, Loss 0.0008, Val Accuracy 0.7380\n",
      "Epoch 26, Loss 0.0003, Val Accuracy 0.7340\n",
      "Epoch 27, Loss 0.0004, Val Accuracy 0.7360\n",
      "Epoch 28, Loss 0.0003, Val Accuracy 0.7360\n",
      "Epoch 29, Loss 0.0006, Val Accuracy 0.7340\n",
      "Epoch 30, Loss 0.0003, Val Accuracy 0.7320\n",
      "Epoch 31, Loss 0.0001, Val Accuracy 0.7320\n",
      "Epoch 32, Loss 0.0001, Val Accuracy 0.7340\n",
      "Epoch 33, Loss 0.0005, Val Accuracy 0.7300\n",
      "Epoch 34, Loss 0.0002, Val Accuracy 0.7320\n",
      "Epoch 35, Loss 0.0002, Val Accuracy 0.7300\n",
      "Epoch 36, Loss 0.0000, Val Accuracy 0.7320\n",
      "Epoch 37, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 38, Loss 0.0000, Val Accuracy 0.7280\n",
      "Epoch 39, Loss 0.0004, Val Accuracy 0.7260\n",
      "Epoch 40, Loss 0.0000, Val Accuracy 0.7220\n",
      "Epoch 41, Loss 0.0000, Val Accuracy 0.7180\n",
      "Epoch 42, Loss 0.0001, Val Accuracy 0.7160\n",
      "Epoch 43, Loss 0.0000, Val Accuracy 0.7120\n",
      "Epoch 44, Loss 0.0001, Val Accuracy 0.7100\n",
      "Epoch 45, Loss 0.0000, Val Accuracy 0.7100\n",
      "Epoch 46, Loss 0.0003, Val Accuracy 0.7100\n",
      "Epoch 47, Loss 0.0001, Val Accuracy 0.7100\n",
      "Epoch 48, Loss 0.0000, Val Accuracy 0.7120\n",
      "Epoch 49, Loss 0.0000, Val Accuracy 0.7120\n",
      "Epoch 50, Loss 0.0000, Val Accuracy 0.7120\n",
      "Epoch 51, Loss 0.0000, Val Accuracy 0.7120\n",
      "Epoch 52, Loss 0.0000, Val Accuracy 0.7120\n",
      "Epoch 53, Loss 0.0000, Val Accuracy 0.7140\n",
      "Epoch 54, Loss 0.0000, Val Accuracy 0.7140\n",
      "Epoch 55, Loss 0.0000, Val Accuracy 0.7160\n",
      "Epoch 56, Loss 0.0001, Val Accuracy 0.7160\n",
      "Epoch 57, Loss 0.0001, Val Accuracy 0.7160\n",
      "Epoch 58, Loss 0.0000, Val Accuracy 0.7200\n",
      "Epoch 59, Loss 0.0000, Val Accuracy 0.7200\n",
      "Epoch 60, Loss 0.0000, Val Accuracy 0.7200\n",
      "Epoch 61, Loss 0.0000, Val Accuracy 0.7220\n",
      "Epoch 62, Loss 0.0000, Val Accuracy 0.7220\n",
      "Epoch 63, Loss 0.0000, Val Accuracy 0.7220\n",
      "Epoch 64, Loss 0.0000, Val Accuracy 0.7220\n",
      "Epoch 65, Loss 0.0000, Val Accuracy 0.7220\n",
      "Epoch 66, Loss 0.0000, Val Accuracy 0.7200\n",
      "Epoch 67, Loss 0.0000, Val Accuracy 0.7180\n",
      "Epoch 68, Loss 0.0000, Val Accuracy 0.7240\n",
      "Epoch 69, Loss 0.0000, Val Accuracy 0.7260\n",
      "Epoch 70, Loss 0.0000, Val Accuracy 0.7280\n",
      "Epoch 71, Loss 0.0000, Val Accuracy 0.7260\n",
      "Epoch 72, Loss 0.0000, Val Accuracy 0.7260\n",
      "Epoch 73, Loss 0.0001, Val Accuracy 0.7260\n",
      "Epoch 74, Loss 0.0000, Val Accuracy 0.7260\n",
      "Epoch 75, Loss 0.0000, Val Accuracy 0.7260\n",
      "Epoch 76, Loss 0.0000, Val Accuracy 0.7260\n",
      "Epoch 77, Loss 0.0000, Val Accuracy 0.7260\n",
      "Epoch 78, Loss 0.0000, Val Accuracy 0.7280\n",
      "Epoch 79, Loss 0.0000, Val Accuracy 0.7280\n",
      "Epoch 80, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 81, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 82, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 83, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 84, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 85, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 86, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 87, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 88, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 89, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 90, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 91, Loss 0.0000, Val Accuracy 0.7320\n",
      "Epoch 92, Loss 0.0000, Val Accuracy 0.7320\n",
      "Epoch 93, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 94, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 95, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 96, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 97, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 98, Loss 0.0000, Val Accuracy 0.7300\n",
      "Epoch 99, Loss 0.0000, Val Accuracy 0.7280\n"
     ]
    }
   ],
   "source": [
    "# 定义优化器\n",
    "optim = torch.optim.Adam(model.parameters(),lr = 0.01)\n",
    "# 定义损失函数\n",
    "lossfunc = nn.CrossEntropyLoss()\n",
    "# 训练模型\n",
    "best_val_acc = 0 # 记录验证集的最好结果\n",
    "best_param = None # 记录最佳参数\n",
    "\n",
    "for epoch in range(100): # 训练100个epoch\n",
    "    model.train()\n",
    "    predict = model(feat,edge_index)\n",
    "    loss = lossfunc(predict[train_mask],label[train_mask])# 使用训练集计算损失\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    # 在验证集上进行评估\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_predict = model(feat,edge_index)[val_mask]\n",
    "        # 计算准确率\n",
    "        acc =  (val_predict.argmax(-1) == label[val_mask]).float().mean()\n",
    "        if acc > best_val_acc:\n",
    "            best_val_acc = acc\n",
    "            best_param =  deepcopy(model.state_dict())\n",
    "    print('Epoch %d, Loss %.4f, Val Accuracy %.4f'%(epoch,loss,acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4、预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7750\n"
     ]
    }
   ],
   "source": [
    "# 载入最优模型\n",
    "model.load_state_dict(best_param)\n",
    "# 测试集评估\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_predict = model(feat,edge_index)[test_mask]\n",
    "    # 计算准确率\n",
    "    acc =  (test_predict.argmax(-1) == label[test_mask]).float().mean()\n",
    "print('Test Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 链接预测任务\n",
    "- 链接预测任务是图数据分析中的一个重要任务，旨在预测图中节点之间是否存在链接。换句话说，它试图预测图中尚未观察到的潜在边缘，或者评估现有边缘的可能性。链接预测在多种应用中都非常重要，如社交网络分析、生物信息学、推荐系统等。\n",
    "- 链接预测任务的输入为节点的特征向量和图的结构，输出为任意两个节点间存在边的概率\n",
    "- 链接预测的评估指标有AUC，Recall，Precision等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 载入数据集\n",
    "  将图上的链接划分为训练集(0.6)，验证集(0.2)，和测试集(0.2),其中训练集的80%用于消息传递，20%用于计算loss训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "dataset = Planetoid(root='data', name='Cora')\n",
    "feat = dataset.data.x \n",
    "edge_index = dataset.data.edge_index\n",
    "\n",
    "# 划分数据集\n",
    "num_edges = edge_index.shape[1]\n",
    "rand_index = torch.randperm(num_edges)\n",
    "edge_index = edge_index[:,rand_index]\n",
    "\n",
    "train_num = int(0.6*num_edges)\n",
    "val_num = int(0.2*num_edges) \n",
    "\n",
    "train_edge = edge_index[:,:train_num]\n",
    "val_edge_label_index = edge_index[:,train_num:train_num+val_num]\n",
    "test_edge_label_index = edge_index[:,train_num+val_num:]\n",
    "\n",
    "index_num = int(0.8*train_num)\n",
    "train_edge_index = train_edge[:,:index_num]\n",
    "train_edge_label_index = train_edge[:,index_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 链接预测任务可以视为一个二分类任务，然而由于graph的稀疏特性，具有大量的负样本，使用全部负样本进行训练不太现实，因此我们需要采样部分负样本进行训练和评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 负采样\n",
    "pos_edge_dict = defaultdict(set)\n",
    "for i in range(train_edge.shape[1]):\n",
    "    pos_edge_dict[train_edge[0,i]].add(train_edge[1,i])\n",
    "    pos_edge_dict[train_edge[1,i]].add(train_edge[0,i])\n",
    "\n",
    "all_pos_dict = defaultdict(set)\n",
    "for i in range(train_edge.shape[1]):\n",
    "    pos_edge_dict[train_edge[0,i]].add(train_edge[1,i])\n",
    "    pos_edge_dict[train_edge[1,i]].add(train_edge[0,i])\n",
    "\n",
    "def negative_sample(pos_edge_dict,num_nodes,num_neg_samples=100):\n",
    "    negative_src = np.random.choice(num_nodes, num_neg_samples)\n",
    "    negative_dst = np.random.choice(num_nodes, num_neg_samples)\n",
    "    for i in range(num_neg_samples):\n",
    "        while negative_dst[i] in pos_edge_dict[negative_src[i]]:\n",
    "            negative_dst[i] = np.random.choice(num_nodes)\n",
    "    return torch.tensor([negative_src.tolist(),negative_dst.tolist()])\n",
    "\n",
    "def get_data(pos_edge,pos_edge_dict):\n",
    "    negative_edge = negative_sample(pos_edge_dict,pos_edge.shape[1])\n",
    "    pos_label = torch.ones_like(pos_edge[0])\n",
    "    neg_label = torch.zeros_like(negative_edge[0])\n",
    "    edge_label_index = torch.cat((pos_edge,negative_edge),axis=-1)\n",
    "    edge_label =  torch.cat((pos_label,neg_label)).float()\n",
    "    return  edge_label_index,edge_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为验证集和测试集进行固定负采样用于评估\n",
    "val_edge_label_index,val_edge_label = get_data(val_edge_label_index,all_pos_dict)\n",
    "test_edge_label_index,test_edge_label = get_data(test_edge_label_index,all_pos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建模型\n",
    "- 不同于节点分类直接使用GNN输出节点类别，链接预测中通常将GNN作为编码器来获取节点的表征，再利用解码器来建模两节点间的连边概率。\n",
    "- 解码器通常为直接计算节点表征的dot product，也可以将节点表征拼接后使用mlp预测。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPredict(nn.Module):\n",
    "    def __init__(self,in_dim,hid_dim,num_layers=2,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = GNN(in_dim,hid_dim,hid_dim,num_layers=num_layers,dropout=dropout)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x,edge_index,predict_edge):\n",
    "        node_rep = self.encoder(x,edge_index)\n",
    "        prob = (node_rep[predict_edge[0]] * node_rep[predict_edge[1]]).sum(dim=1)     \n",
    "        return prob \n",
    "\n",
    "model = LinkPredict(feat.shape[1],64,num_layers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 0.6923, Val AUC 0.6352\n",
      "Epoch 1, Loss 0.3593, Val AUC 0.6351\n",
      "Epoch 2, Loss 0.7134, Val AUC 0.6401\n",
      "Epoch 3, Loss 0.3675, Val AUC 0.6544\n",
      "Epoch 4, Loss 0.4395, Val AUC 0.6648\n",
      "Epoch 5, Loss 0.5163, Val AUC 0.6677\n",
      "Epoch 6, Loss 0.4819, Val AUC 0.6662\n",
      "Epoch 7, Loss 0.3931, Val AUC 0.6627\n",
      "Epoch 8, Loss 0.3005, Val AUC 0.6591\n",
      "Epoch 9, Loss 0.2998, Val AUC 0.6574\n",
      "Epoch 10, Loss 0.3543, Val AUC 0.6584\n",
      "Epoch 11, Loss 0.3821, Val AUC 0.6620\n",
      "Epoch 12, Loss 0.2965, Val AUC 0.6662\n",
      "Epoch 13, Loss 0.2753, Val AUC 0.6714\n",
      "Epoch 14, Loss 0.2687, Val AUC 0.6768\n",
      "Epoch 15, Loss 0.2613, Val AUC 0.6809\n",
      "Epoch 16, Loss 0.2672, Val AUC 0.6844\n",
      "Epoch 17, Loss 0.2713, Val AUC 0.6871\n",
      "Epoch 18, Loss 0.2676, Val AUC 0.6897\n",
      "Epoch 19, Loss 0.2569, Val AUC 0.6910\n",
      "Epoch 20, Loss 0.2524, Val AUC 0.6918\n",
      "Epoch 21, Loss 0.2413, Val AUC 0.6926\n",
      "Epoch 22, Loss 0.2385, Val AUC 0.6933\n",
      "Epoch 23, Loss 0.2455, Val AUC 0.6946\n",
      "Epoch 24, Loss 0.2668, Val AUC 0.6963\n",
      "Epoch 25, Loss 0.2471, Val AUC 0.6980\n",
      "Epoch 26, Loss 0.2428, Val AUC 0.7003\n",
      "Epoch 27, Loss 0.2391, Val AUC 0.7018\n",
      "Epoch 28, Loss 0.2370, Val AUC 0.7036\n",
      "Epoch 29, Loss 0.2419, Val AUC 0.7049\n",
      "Epoch 30, Loss 0.2389, Val AUC 0.7050\n",
      "Epoch 31, Loss 0.2433, Val AUC 0.7057\n",
      "Epoch 32, Loss 0.2433, Val AUC 0.7062\n",
      "Epoch 33, Loss 0.2425, Val AUC 0.7057\n",
      "Epoch 34, Loss 0.2471, Val AUC 0.7052\n",
      "Epoch 35, Loss 0.2429, Val AUC 0.7048\n",
      "Epoch 36, Loss 0.2466, Val AUC 0.7046\n",
      "Epoch 37, Loss 0.2452, Val AUC 0.7047\n",
      "Epoch 38, Loss 0.2391, Val AUC 0.7037\n",
      "Epoch 39, Loss 0.2408, Val AUC 0.7029\n",
      "Epoch 40, Loss 0.2320, Val AUC 0.7016\n",
      "Epoch 41, Loss 0.2375, Val AUC 0.7007\n",
      "Epoch 42, Loss 0.2411, Val AUC 0.6994\n",
      "Epoch 43, Loss 0.2436, Val AUC 0.6986\n",
      "Epoch 44, Loss 0.2376, Val AUC 0.6971\n",
      "Epoch 45, Loss 0.2436, Val AUC 0.6955\n",
      "Epoch 46, Loss 0.2365, Val AUC 0.6944\n",
      "Epoch 47, Loss 0.2354, Val AUC 0.6929\n",
      "Epoch 48, Loss 0.2372, Val AUC 0.6924\n",
      "Epoch 49, Loss 0.2377, Val AUC 0.6916\n",
      "Epoch 50, Loss 0.2353, Val AUC 0.6911\n",
      "Epoch 51, Loss 0.2412, Val AUC 0.6905\n",
      "Epoch 52, Loss 0.2421, Val AUC 0.6908\n",
      "Epoch 53, Loss 0.2379, Val AUC 0.6912\n",
      "Epoch 54, Loss 0.2303, Val AUC 0.6905\n",
      "Epoch 55, Loss 0.2363, Val AUC 0.6901\n",
      "Epoch 56, Loss 0.2336, Val AUC 0.6899\n",
      "Epoch 57, Loss 0.2377, Val AUC 0.6899\n",
      "Epoch 58, Loss 0.2356, Val AUC 0.6906\n",
      "Epoch 59, Loss 0.2353, Val AUC 0.6909\n",
      "Epoch 60, Loss 0.2415, Val AUC 0.6917\n",
      "Epoch 61, Loss 0.2321, Val AUC 0.6919\n",
      "Epoch 62, Loss 0.2360, Val AUC 0.6925\n",
      "Epoch 63, Loss 0.2387, Val AUC 0.6931\n",
      "Epoch 64, Loss 0.2282, Val AUC 0.6941\n",
      "Epoch 65, Loss 0.2322, Val AUC 0.6948\n",
      "Epoch 66, Loss 0.2384, Val AUC 0.6950\n",
      "Epoch 67, Loss 0.2289, Val AUC 0.6954\n",
      "Epoch 68, Loss 0.2310, Val AUC 0.6953\n",
      "Epoch 69, Loss 0.2353, Val AUC 0.6963\n",
      "Epoch 70, Loss 0.2343, Val AUC 0.6969\n",
      "Epoch 71, Loss 0.2309, Val AUC 0.6973\n",
      "Epoch 72, Loss 0.2185, Val AUC 0.6978\n",
      "Epoch 73, Loss 0.2263, Val AUC 0.6984\n",
      "Epoch 74, Loss 0.2288, Val AUC 0.6982\n",
      "Epoch 75, Loss 0.2312, Val AUC 0.6981\n",
      "Epoch 76, Loss 0.2310, Val AUC 0.6978\n",
      "Epoch 77, Loss 0.2258, Val AUC 0.6972\n",
      "Epoch 78, Loss 0.2354, Val AUC 0.6958\n",
      "Epoch 79, Loss 0.2276, Val AUC 0.6955\n",
      "Epoch 80, Loss 0.2251, Val AUC 0.6948\n",
      "Epoch 81, Loss 0.2252, Val AUC 0.6948\n",
      "Epoch 82, Loss 0.2320, Val AUC 0.6947\n",
      "Epoch 83, Loss 0.2179, Val AUC 0.6954\n",
      "Epoch 84, Loss 0.2174, Val AUC 0.6964\n",
      "Epoch 85, Loss 0.2267, Val AUC 0.6966\n",
      "Epoch 86, Loss 0.2278, Val AUC 0.6970\n",
      "Epoch 87, Loss 0.2300, Val AUC 0.6958\n",
      "Epoch 88, Loss 0.2201, Val AUC 0.6966\n",
      "Epoch 89, Loss 0.2158, Val AUC 0.6968\n",
      "Epoch 90, Loss 0.2228, Val AUC 0.6986\n",
      "Epoch 91, Loss 0.2225, Val AUC 0.7009\n",
      "Epoch 92, Loss 0.2238, Val AUC 0.7020\n",
      "Epoch 93, Loss 0.2040, Val AUC 0.7040\n",
      "Epoch 94, Loss 0.2153, Val AUC 0.7059\n",
      "Epoch 95, Loss 0.2129, Val AUC 0.7087\n",
      "Epoch 96, Loss 0.2112, Val AUC 0.7122\n",
      "Epoch 97, Loss 0.2073, Val AUC 0.7148\n",
      "Epoch 98, Loss 0.2111, Val AUC 0.7245\n",
      "Epoch 99, Loss 0.2017, Val AUC 0.7301\n"
     ]
    }
   ],
   "source": [
    "# 定义优化器\n",
    "optim = torch.optim.Adam(model.parameters(),lr = 0.01)\n",
    "# 定义损失函数\n",
    "lossfunc = nn.BCEWithLogitsLoss()\n",
    "# 训练模型\n",
    "best_val_auc = 0 # 记录验证集的最好结果\n",
    "best_param = None # 记录最佳参数\n",
    "\n",
    "for epoch in range(100): # 训练100个epoch\n",
    "    model.train()\n",
    "    edge_label_index,edge_label = get_data(train_edge_label_index,pos_edge_dict)\n",
    "    predict = model(feat,train_edge_index,edge_label_index)\n",
    "    loss = lossfunc(predict,edge_label)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    # 在验证集上进行评估\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predict = model(feat,train_edge_index,val_edge_label_index)\n",
    "        auc = roc_auc_score(val_edge_label.long().numpy(),predict.numpy())\n",
    "        if auc > best_val_auc:\n",
    "            best_val_auc = auc\n",
    "            best_param =  deepcopy(model.state_dict())\n",
    "    print('Epoch %d, Loss %.4f, Val AUC %.4f'%(epoch,loss,auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在测试集上评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.6643\n"
     ]
    }
   ],
   "source": [
    "# 载入最优模型\n",
    "model.load_state_dict(best_param)\n",
    "# 测试集评估\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predict = model(feat,train_edge_index,test_edge_label_index)\n",
    "    auc = roc_auc_score(test_edge_label.long().numpy(),predict.numpy())\n",
    "print('Test AUC: {:.4f}'.format(auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
